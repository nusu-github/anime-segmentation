# IBIS-Net Base Configuration
# ============================
# This is the default configuration for training ISNet models.
# Use this as a base and override specific values in experiment variants.
#
# Usage:
#   python -m ibis_net.train_cli fit --config configs/base.yaml
#
# To resume training from a checkpoint:
#   python -m ibis_net.train_cli fit --config configs/base.yaml --ckpt_path path/to/checkpoint.ckpt
#
# To override specific values:
#   python -m ibis_net.train_cli fit --config configs/base.yaml --trainer.max_epochs=500
#
# Callback Configuration:
# -----------------------
# Callbacks are configured under trainer.callbacks as a list of class_path/init_args pairs.
# You can add custom callbacks by adding more entries to the list:
#
#   trainer:
#     callbacks:
#       - class_path: lightning.pytorch.callbacks.LearningRateMonitor
#         init_args:
#           logging_interval: step
#       - class_path: your.custom.Callback
#         init_args:
#           param1: value1

# Random seed for reproducibility
seed_everything: 42

# =============================================================================
# Trainer Configuration
# =============================================================================
trainer:
  # Training duration
  max_epochs: 1000
  max_steps: -1  # -1 means unlimited

  # Hardware settings
  accelerator: auto
  devices: auto
  precision: 16-mixed

  # Logging
  log_every_n_steps: 20
  default_root_dir: saved_models/

  # W&B Logger for experiment tracking
  # Logs metrics, hyperparameters, and model artifacts to Weights & Biases
  # Set WANDB_API_KEY environment variable or run `wandb login` before training
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: ibis-net
      save_dir: saved_models/
      log_model: false  # Set to true to save model checkpoints to W&B

  # Callbacks for checkpointing and early stopping
  callbacks:
    # ModelCheckpoint: Save best and last checkpoints
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min
        save_top_k: 1
        save_last: true
        every_n_train_steps: 2000
        filename: "{epoch}-{val_loss:.4f}"

    # EarlyStopping: Stop training when validation loss plateaus
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        patience: 20
        mode: min
        verbose: true

    # LearningRateMonitor: Log learning rate to W&B
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

# =============================================================================
# Model Configuration
# =============================================================================
# With subclass_mode_model=True, we use class_path/init_args to specify the LightningModule.
model:
  class_path: ibis_net.lightning.ISNetLightningModule
  init_args:
    # Model architecture (injected via class_path)
    model:
      class_path: ibis_net.models.ISNetDIS
      init_args:
        in_ch: 3
        out_ch: 1

    # Intermediate supervision settings
    # Set interm_sup: true and provide gt_encoder to enable
    interm_sup: false
    fs_loss_mode: MSE

    # Optimizer configuration using class_path/init_args pattern
    # This allows switching optimizers without code changes
    optimizer:
      class_path: torch.optim.Adam
      init_args:
        lr: 0.001
        betas: [0.9, 0.999]
        weight_decay: 0.0

    # Metrics configuration
    enable_metrics: true
    metric_names:
      - F
      - WF
      - MAE
      - S
      - HCE
      - MBA
      - BIoU

    # Normalization (must match DataModule settings)
    normalize_mean: [0.485, 0.456, 0.406]
    normalize_std: [0.229, 0.224, 0.225]

    # torch.compile settings for model acceleration
    # Requires PyTorch 2.0+. First epoch will be slower due to compilation.
    compile_model: false
    # Mode options: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs
    compile_mode: default
    compile_fullgraph: true
    compile_dynamic: null
    compile_backend: inductor

# =============================================================================
# Data Configuration
# =============================================================================
data:
  # Training datasets
  # Each dataset requires: name, im_dir, gt_dir, im_ext, gt_ext
  train_datasets:
    - name: DIS5K-TR
      im_dir: datasets/DIS5K/DIS-TR/im
      gt_dir: datasets/DIS5K/DIS-TR/gt
      im_ext: .jpg
      gt_ext: .png

  # Validation datasets
  val_datasets:
    - name: DIS5K-VD
      im_dir: datasets/DIS5K/DIS-VD/im
      gt_dir: datasets/DIS5K/DIS-VD/gt
      im_ext: .jpg
      gt_ext: .png

  # Test datasets (used with 'test' subcommand)
  test_datasets: []

  # Prediction datasets (used with 'predict' subcommand)
  predict_datasets: []

  # Image settings
  image_size: [1024, 1024]

  # Batch sizes
  batch_size_train: 8
  batch_size_valid: 1

  # Normalization (used for validation/test; training uses GPU augmentation)
  # ImageNet mean/std
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

  # Augmentation preset for GPU-based training augmentation
  # Options: default, light, aggressive, none
  aug_preset_train: default

  # DataLoader workers
  num_workers: 4
