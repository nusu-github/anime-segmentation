# GT Encoder Training Configuration
# ==================================
# This configuration trains the GT encoder (ISNetGTEncoder) which learns to
# encode ground truth masks. The trained encoder is then used for intermediate
# supervision in the main ISNet training (B0 baseline).
#
# Usage:
#   python -m ibis_net.train_cli fit --config configs/base.yaml --config configs/experiments/gt_encoder_train.yaml
#
# After training, use the checkpoint for B0 baseline:
#   python -m ibis_net.train_cli fit \
#     --config configs/base.yaml \
#     --config configs/experiments/b0_baseline.yaml \
#     --model.gt_encoder.init_args.ckpt_path=saved_models/gt_encoder/last.ckpt
#
# Note: GT encoder learns to reconstruct GT masks, training only on label images.

# Override model to use GTEncoderLightningModule
model:
  class_path: ibis_net.lightning.GTEncoderLightningModule
  init_args:
    # GT encoder model architecture
    model:
      class_path: ibis_net.models.ISNetGTEncoder
      init_args:
        in_ch: 1  # GT masks are single channel
        out_ch: 1

    # Optimizer configuration
    optimizer:
      class_path: torch.optim.Adam
      init_args:
        lr: 0.001
        betas: [0.9, 0.999]
        weight_decay: 0.0

data:
  # Horizontal flip only (consistent with B0 baseline)
  aug_preset_train: hflip_only

# Trainer overrides for GT encoder training
trainer:
  # GT encoder typically converges faster
  max_epochs: 500

  # Output directory
  default_root_dir: saved_models/gt_encoder/

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min
        save_top_k: 1
        save_last: true
        every_n_train_steps: 2000
        filename: "gt_encoder-{epoch}-{val_loss:.4f}"

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        patience: 30
        mode: min
        verbose: true
