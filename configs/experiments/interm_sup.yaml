# Intermediate Supervision Experiment Variant
# ============================================
# This configuration enables intermediate supervision with GT encoder
# for improved feature learning through knowledge distillation.
#
# Usage:
#   python -m ibis_net.train_cli fit --config configs/base.yaml --config configs/experiments/interm_sup.yaml
#
# Note: The GT encoder must be pre-trained separately or loaded from checkpoint.
# To train GT encoder first:
#   python -m ibis_net.train_cli fit --config configs/base.yaml --config configs/experiments/gt_encoder_train.yaml
#
# Then specify the checkpoint path below or via CLI:
#   python -m ibis_net.train_cli fit \
#     --config configs/base.yaml \
#     --config configs/experiments/interm_sup.yaml \
#     --model.init_args.gt_encoder.init_args.ckpt_path=saved_models/gt_encoder/last.ckpt

model:
  init_args:
    # Enable intermediate supervision
    interm_sup: true

    # GT encoder for feature matching (injected via class_path)
    gt_encoder:
      class_path: ibis_net.models.ISNetGTEncoder
      init_args:
        in_ch: 1  # GT masks are single channel
        out_ch: 1
        # Uncomment and set path to trained GT encoder checkpoint:
        # ckpt_path: saved_models/gt_encoder/last.ckpt

    # Feature matching loss mode
    fs_loss_mode: MSE

# Override checkpoint monitoring to use maxF metric
trainer:
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_maxF
        mode: max
        save_top_k: 1
        save_last: true
        filename: "{epoch}-{val_maxF:.4f}"

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_maxF
        patience: 20
        mode: max
        verbose: true
