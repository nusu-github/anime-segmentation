# Codes are borrowed from
# https://github.com/xuebinqin/DIS/blob/main/IS-Net/models/isnet.py

import torch
import torch.nn.functional as F
from torch import nn


def _resize_target_like(target: torch.Tensor, pred: torch.Tensor) -> torch.Tensor:
    if pred.shape[2:] == target.shape[2:]:
        return target
    return F.interpolate(target, size=pred.shape[2:], mode="bilinear", align_corners=True)


def _bce_with_logits(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    return F.binary_cross_entropy_with_logits(pred, target, reduction="mean")


def _feature_loss(pred: torch.Tensor, target: torch.Tensor, mode: str) -> torch.Tensor:
    if mode == "MSE":
        return F.mse_loss(pred, target, reduction="mean")
    if mode == "KL":
        return F.kl_div(
            F.log_softmax(pred, dim=1),
            F.softmax(target, dim=1),
            reduction="mean",
        )
    if mode == "MAE":
        return F.l1_loss(pred, target, reduction="mean")
    if mode == "SmoothL1":
        return F.smooth_l1_loss(pred, target, reduction="mean")
    raise ValueError(f"Unsupported feature loss mode: {mode}")


def multi_loss_fusion(
    preds: list[torch.Tensor], target: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    loss0 = torch.zeros(1, dtype=target.dtype, device=target.device)
    loss = torch.zeros(1, dtype=target.dtype, device=target.device)

    for i, pred in enumerate(preds):
        resized_target = _resize_target_like(target, pred)
        loss = loss + _bce_with_logits(pred, resized_target)
        if i == 0:
            loss0 = loss
    return loss0, loss


def multi_loss_fusion_kl(
    preds: list[torch.Tensor],
    target: torch.Tensor,
    dfs: list[torch.Tensor],
    fs: list[torch.Tensor],
    mode: str = "MSE",
) -> tuple[torch.Tensor, torch.Tensor]:
    loss0 = torch.zeros(1, dtype=target.dtype, device=target.device)
    loss = torch.zeros(1, dtype=target.dtype, device=target.device)

    for i, pred in enumerate(preds):
        resized_target = _resize_target_like(target, pred)
        loss = loss + _bce_with_logits(pred, resized_target)
        if i == 0:
            loss0 = loss

    for df, fs_i in zip(dfs, fs, strict=True):
        loss = loss + _feature_loss(df, fs_i, mode)

    return loss0, loss


class RebnConv(nn.Module):
    def __init__(self, in_ch: int = 3, out_ch: int = 3, dirate: int = 1, stride: int = 1):
        super().__init__()

        self.conv_s1 = nn.Conv2d(
            in_ch, out_ch, 3, padding=1 * dirate, dilation=1 * dirate, stride=stride
        )
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))


## upsample tensor 'src' to have the same spatial size with tensor 'tar'
def _upsample_like(src: torch.Tensor, tar: torch.Tensor) -> torch.Tensor:
    return F.interpolate(src, size=tar.shape[2:], mode="bilinear", align_corners=False)


### RSU-7 ###
class RSU7(nn.Module):
    def __init__(self, in_ch: int = 3, mid_ch: int = 12, out_ch: int = 3, img_size: int = 512):
        super().__init__()

        self.in_ch = in_ch
        self.mid_ch = mid_ch
        self.out_ch = out_ch

        self.rebnconvin = RebnConv(in_ch, out_ch, dirate=1)  ## 1 -> 1/2

        self.rebnconv1 = RebnConv(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv5 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv6 = RebnConv(mid_ch, mid_ch, dirate=1)

        self.rebnconv7 = RebnConv(mid_ch, mid_ch, dirate=2)

        self.rebnconv6d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv5d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = RebnConv(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hxin = self.rebnconvin(x)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)

        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)

        hx6 = self.rebnconv6(hx)

        hx7 = self.rebnconv7(hx6)

        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = _upsample_like(hx6d, hx5)

        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-6 ###
class RSU6(nn.Module):
    def __init__(self, in_ch: int = 3, mid_ch: int = 12, out_ch: int = 3):
        super().__init__()

        self.rebnconvin = RebnConv(in_ch, out_ch, dirate=1)

        self.rebnconv1 = RebnConv(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv5 = RebnConv(mid_ch, mid_ch, dirate=1)

        self.rebnconv6 = RebnConv(mid_ch, mid_ch, dirate=2)

        self.rebnconv5d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = RebnConv(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hxin = self.rebnconvin(x)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)

        hx5 = self.rebnconv5(hx)

        hx6 = self.rebnconv6(hx5)

        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-5 ###
class RSU5(nn.Module):
    def __init__(self, in_ch: int = 3, mid_ch: int = 12, out_ch: int = 3):
        super().__init__()

        self.rebnconvin = RebnConv(in_ch, out_ch, dirate=1)

        self.rebnconv1 = RebnConv(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = RebnConv(mid_ch, mid_ch, dirate=1)

        self.rebnconv5 = RebnConv(mid_ch, mid_ch, dirate=2)

        self.rebnconv4d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = RebnConv(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hxin = self.rebnconvin(x)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)

        hx5 = self.rebnconv5(hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-4 ###
class RSU4(nn.Module):
    def __init__(self, in_ch: int = 3, mid_ch: int = 12, out_ch: int = 3):
        super().__init__()

        self.rebnconvin = RebnConv(in_ch, out_ch, dirate=1)

        self.rebnconv1 = RebnConv(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = RebnConv(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = RebnConv(mid_ch, mid_ch, dirate=1)

        self.rebnconv4 = RebnConv(mid_ch, mid_ch, dirate=2)

        self.rebnconv3d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = RebnConv(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = RebnConv(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hxin = self.rebnconvin(x)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)

        hx4 = self.rebnconv4(hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-4F ###
class RSU4F(nn.Module):
    def __init__(self, in_ch: int = 3, mid_ch: int = 12, out_ch: int = 3):
        super().__init__()

        self.rebnconvin = RebnConv(in_ch, out_ch, dirate=1)

        self.rebnconv1 = RebnConv(out_ch, mid_ch, dirate=1)
        self.rebnconv2 = RebnConv(mid_ch, mid_ch, dirate=2)
        self.rebnconv3 = RebnConv(mid_ch, mid_ch, dirate=4)

        self.rebnconv4 = RebnConv(mid_ch, mid_ch, dirate=8)

        self.rebnconv3d = RebnConv(mid_ch * 2, mid_ch, dirate=4)
        self.rebnconv2d = RebnConv(mid_ch * 2, mid_ch, dirate=2)
        self.rebnconv1d = RebnConv(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hxin = self.rebnconvin(x)

        hx1 = self.rebnconv1(hxin)
        hx2 = self.rebnconv2(hx1)
        hx3 = self.rebnconv3(hx2)

        hx4 = self.rebnconv4(hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx2d = self.rebnconv2d(torch.cat((hx3d, hx2), 1))
        hx1d = self.rebnconv1d(torch.cat((hx2d, hx1), 1))

        return hx1d + hxin


class MyRebnConv(nn.Module):
    def __init__(
        self,
        in_ch: int = 3,
        out_ch: int = 1,
        kernel_size: int = 3,
        stride: int = 1,
        padding: int = 1,
        dilation: int = 1,
        groups: int = 1,
    ):
        super().__init__()

        self.conv = nn.Conv2d(
            in_ch,
            out_ch,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )
        self.bn = nn.BatchNorm2d(out_ch)
        self.rl = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.rl(self.bn(self.conv(x)))


class ISNetGTEncoder(nn.Module):
    def __init__(
        self,
        in_ch: int = 1,
        out_ch: int = 1,
    ):
        super().__init__()

        self.conv_in = MyRebnConv(
            in_ch, 16, 3, stride=2, padding=1
        )  # nn.Conv2d(in_ch,64,3,stride=2,padding=1)

        self.stage1 = RSU7(16, 16, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage2 = RSU6(64, 16, 64)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage3 = RSU5(64, 32, 128)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage4 = RSU4(128, 32, 256)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage5 = RSU4F(256, 64, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage6 = RSU4F(512, 64, 512)

        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)

    @staticmethod
    def compute_loss(
        args: tuple[list[torch.Tensor], torch.Tensor],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        preds, targets = args
        return multi_loss_fusion(preds, targets)

    def forward(self, x: torch.Tensor) -> tuple[list[torch.Tensor], list[torch.Tensor]]:
        hxin = self.conv_in(x)

        # stage 1
        hx1 = self.stage1(hxin)
        hx = self.pool12(hx1)

        # stage 2
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)

        # stage 3
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)

        # stage 4
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)

        # stage 5
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)

        # stage 6
        hx6 = self.stage6(hx)

        # side output
        d1 = self.side1(hx1)
        d1 = _upsample_like(d1, x)

        d2 = self.side2(hx2)
        d2 = _upsample_like(d2, x)

        d3 = self.side3(hx3)
        d3 = _upsample_like(d3, x)

        d4 = self.side4(hx4)
        d4 = _upsample_like(d4, x)

        d5 = self.side5(hx5)
        d5 = _upsample_like(d5, x)

        d6 = self.side6(hx6)
        d6 = _upsample_like(d6, x)

        return [d1, d2, d3, d4, d5, d6], [hx1, hx2, hx3, hx4, hx5, hx6]


class ISNetDIS(nn.Module):
    def __init__(
        self,
        in_ch: int = 3,
        out_ch: int = 1,
    ):
        super().__init__()

        self.conv_in = nn.Conv2d(in_ch, 64, 3, stride=2, padding=1)
        self.pool_in = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage1 = RSU7(64, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage2 = RSU6(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage3 = RSU5(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage4 = RSU4(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage5 = RSU4F(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage6 = RSU4F(512, 256, 512)

        # decoder
        self.stage5d = RSU4F(1024, 256, 512)
        self.stage4d = RSU4(1024, 128, 256)
        self.stage3d = RSU5(512, 64, 128)
        self.stage2d = RSU6(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)

        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)

    @staticmethod
    def compute_loss_kl(
        preds: list[torch.Tensor],
        targets: torch.Tensor,
        dfs: list[torch.Tensor],
        fs: list[torch.Tensor],
        mode: str = "MSE",
    ) -> tuple[torch.Tensor, torch.Tensor]:
        return multi_loss_fusion_kl(preds, targets, dfs, fs, mode=mode)

    @staticmethod
    def compute_loss(args: tuple) -> tuple[torch.Tensor, torch.Tensor]:
        if len(args) == 3:
            ds, _dfs, labels = args
            return multi_loss_fusion(ds, labels)
        ds, dfs, labels, fs = args
        return multi_loss_fusion_kl(ds, labels, dfs, fs, mode="MSE")

    def forward(self, x: torch.Tensor) -> tuple[list[torch.Tensor], list[torch.Tensor]]:
        hxin = self.conv_in(x)
        hx = self.pool_in(hxin)

        # stage 1
        hx1 = self.stage1(hxin)
        hx = self.pool12(hx1)

        # stage 2
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)

        # stage 3
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)

        # stage 4
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)

        # stage 5
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)

        # stage 6
        hx6 = self.stage6(hx)
        hx6up = _upsample_like(hx6, hx5)

        # -------------------- decoder --------------------
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)

        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))

        # side output
        d1 = self.side1(hx1d)
        d1 = _upsample_like(d1, x)

        d2 = self.side2(hx2d)
        d2 = _upsample_like(d2, x)

        d3 = self.side3(hx3d)
        d3 = _upsample_like(d3, x)

        d4 = self.side4(hx4d)
        d4 = _upsample_like(d4, x)

        d5 = self.side5(hx5d)
        d5 = _upsample_like(d5, x)

        d6 = self.side6(hx6)
        d6 = _upsample_like(d6, x)

        return [d1, d2, d3, d4, d5, d6], [hx1d, hx2d, hx3d, hx4d, hx5d, hx6]
